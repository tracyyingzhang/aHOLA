{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b209b507-9882-48bd-8c68-29366a98c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "# Optimized LangevinSampler class\n",
    "class LangevinSampler:\n",
    "    def __init__(self, targ, algo, step=0.001, beta=1, d=None, n=None, gamma=None):\n",
    "        assert targ == 'posterior'\n",
    "        assert algo in ['LMC', 'LMCO', 'aHOLLA']\n",
    "        \n",
    "        self.targ = targ\n",
    "        self.algo = algo\n",
    "        self.step = step\n",
    "        self.beta = beta\n",
    "        self.d = d\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Precompute eye matrix for cov\n",
    "        self.cov = np.eye(d)\n",
    "        \n",
    "        # Cache variables\n",
    "        self._exp_cache = None\n",
    "        self._grad_cache = None\n",
    "        self._hvp_cache = None\n",
    "\n",
    "    def _exp(self, theta, x):\n",
    "        z = x[:, :-1]\n",
    "        if self._exp_cache is None:\n",
    "            self._exp_cache = np.exp(np.dot(z, theta))\n",
    "        return self._exp_cache\n",
    "\n",
    "    def _gradient(self, theta, x):\n",
    "        z, y = x[:, :-1], x[:, -1]\n",
    "        exp_term = self._exp(theta, x)\n",
    "        \n",
    "        term1 = np.sum((1 - y[:, np.newaxis]) * z, axis=0)\n",
    "        term2 = np.sum(z / (1 + exp_term)[:, np.newaxis], axis=0)\n",
    "        term3 = self.gamma * theta  # Since cov is eye matrix\n",
    "        \n",
    "        return term1 - term2 + term3\n",
    "\n",
    "    def _hessianvectorproduct(self, theta, x, vector):\n",
    "        z = x[:, :-1]\n",
    "        exp_term = self._exp(theta, x)\n",
    "        \n",
    "        term = exp_term / (1 + exp_term) ** 2\n",
    "        hess_inner = term * np.dot(z, vector)\n",
    "        hvp_term = np.dot(z.T, hess_inner)\n",
    "        \n",
    "        return self.gamma * vector + hvp_term\n",
    "\n",
    "    def _vectorlaplacian(self, theta, x):\n",
    "        z = x[:, :-1]\n",
    "        exp_term = self._exp(theta, x)\n",
    "        \n",
    "        norms_squared = np.sum(z**2, axis=1) \n",
    "        term = exp_term * (1 - exp_term) / (1 + exp_term)**3\n",
    "        return np.sum(z * (norms_squared * term)[:, np.newaxis], axis=0)\n",
    "\n",
    "    def _hessian_term(self, theta, x):\n",
    "        if self.algo == 'LMC':\n",
    "            return 0\n",
    "        return self._hessianvectorproduct(theta, x, self._gradient(theta, x))\n",
    "\n",
    "    def _vectorlaplacian_term(self, theta, x):\n",
    "        if self.algo == 'aHOLLA':\n",
    "            return self._vectorlaplacian(theta, x) / self.beta \n",
    "        return 0\n",
    "\n",
    "    def _diffusion_term(self, theta, x):\n",
    "        if self.algo == 'LMC':\n",
    "            return np.random.standard_normal(self.d)\n",
    "        \n",
    "        gau_a, gau_b = np.random.standard_normal((2, self.d))\n",
    "        \n",
    "        hvp_a = self._hessianvectorproduct(theta, x, gau_a)\n",
    "        hvp_b = self._hessianvectorproduct(theta, x, gau_b)\n",
    "        \n",
    "        term_a = gau_a - self.step * hvp_a / 2\n",
    "        term_b = (np.sqrt(3) / 6) * self.step * hvp_b\n",
    "        \n",
    "        return term_a + term_b\n",
    "\n",
    "    def sample(self, theta0, x, runtime=200):\n",
    "        n_iter = int(runtime/self.step)\n",
    "        theta = np.ravel(np.array(theta0).reshape(-1))\n",
    "        theta_arr = np.zeros((200, self.d))\n",
    "        \n",
    "        for n in range(n_iter):\n",
    "            # Clear cache at start of each iteration\n",
    "            self._exp_cache = None\n",
    "            \n",
    "            grad = self._gradient(theta, x)\n",
    "            hess_term = self._hessian_term(theta, x)\n",
    "            vec_lap_term = self._vectorlaplacian_term(theta, x)\n",
    "            diffusion = self._diffusion_term(theta, x)\n",
    "            \n",
    "            # Update theta\n",
    "            theta += (-self.step * grad + \n",
    "                     (self.step**2 / 2) * hess_term - \n",
    "                     (self.step**2 / 2) * vec_lap_term + \n",
    "                     np.sqrt(2 * self.step / self.beta) * diffusion)\n",
    "            \n",
    "            # Store last 200 samples\n",
    "            if n >= (n_iter - 200):\n",
    "                theta_arr[n - (n_iter - 200)] = theta\n",
    "                \n",
    "        return theta_arr\n",
    "\n",
    "def generate_data(n, d, theta_true):\n",
    "    z = np.random.choice([-1, 1], size=(n, d))\n",
    "    p = 1 / (1 + np.exp(-np.dot(z, theta_true)))\n",
    "    y = np.random.binomial(1, p)\n",
    "    return np.hstack((z, y.reshape(-1, 1)))\n",
    "\n",
    "def draw_samples_parallel(sampler, theta0, runtime=200, n_chains=50, n_jobs=-1):\n",
    "    d = len(np.ravel(theta0))\n",
    "    sampler.d = d\n",
    "    \n",
    "    def _run_single_markov_chain():\n",
    "        x = generate_data(sampler.n, d, np.ones(d))\n",
    "        return pd.DataFrame(\n",
    "            sampler.sample(theta0, x, runtime=runtime),\n",
    "            columns=[f'component_{i+1}' for i in range(d)]\n",
    "        )\n",
    "    \n",
    "    samples_df = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "        delayed(_run_single_markov_chain)() for _ in range(n_chains)\n",
    "    )\n",
    "    \n",
    "    return pd.concat(samples_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53fa79-2e49-48bd-8763-b81454cf36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_values = [100, 200, 300, 400, 500]\n",
    "    d_values = [2, 5, 10]\n",
    "    runtime = 1000\n",
    "    n_chains = 50\n",
    "    step_size = 0.01\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for d in d_values:\n",
    "        for n in n_values:\n",
    "            theta_true = np.ones(d)\n",
    "            gamma = 1\n",
    "            \n",
    "            for algo in ['LMC', 'LMCO', 'aHOLLA']:\n",
    "                sampler = LangevinSampler(\n",
    "                    targ='posterior', \n",
    "                    algo=algo, \n",
    "                    step=step_size, \n",
    "                    d=d, \n",
    "                    n=n, \n",
    "                    gamma=gamma\n",
    "                )\n",
    "                \n",
    "                start_time = time.time()\n",
    "                samples_df = draw_samples_parallel(\n",
    "                    sampler, \n",
    "                    2 * np.ones(d), \n",
    "                    runtime=runtime, \n",
    "                    n_chains=n_chains\n",
    "                )\n",
    "                runtime_sec = time.time() - start_time\n",
    "                \n",
    "                # Compute MSE\n",
    "                mean_samples = samples_df.groupby(samples_df.index // 200).mean().values\n",
    "                mse = np.mean(np.linalg.norm(mean_samples - theta_true, axis=1) ** 2)\n",
    "                \n",
    "                results[(d, n, algo)] = {\n",
    "                    'mse': mse,\n",
    "                    'runtime': runtime_sec\n",
    "                }\n",
    "                \n",
    "                print(f\"d={d}, n={n}, {algo}: MSE={mse:.4f}, Time={runtime_sec:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
